# Dokumentasi Lengkap: main.py

## Overview

File `main.py` adalah **entry point** dari backend API yang dibangun menggunakan **FastAPI framework**. File ini bertanggung jawab untuk:

1. Menerima HTTP requests dari frontend
2. Mengolah data dengan memanggil modul-modul pemrosesan
3. Mengirim response kembali ke frontend

---

## Import Libraries

```python
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional, Dict
import os
import json
import asyncio
```

### Penjelasan Detail:

**1. `from fastapi import FastAPI, HTTPException`**

- `FastAPI`: Class utama untuk membuat aplikasi web API
- `HTTPException`: Class untuk mengirim error HTTP dengan status code tertentu (400, 404, 500, dll)

**2. `from fastapi.middleware.cors import CORSMiddleware`**

- CORS (Cross-Origin Resource Sharing) middleware
- Diperlukan karena frontend (localhost:3000) dan backend (localhost:8000) berjalan di port berbeda
- Tanpa CORS, browser akan memblokir request dari frontend ke backend

**3. `from fastapi.responses import StreamingResponse`**

- Digunakan untuk mengirim data secara **bertahap** (streaming)
- Berguna untuk mengirim progress update saat memproses 100 file
- Data dikirim menggunakan **Server-Sent Events (SSE)**

**4. `from pydantic import BaseModel`**

- Pydantic adalah library untuk validasi data
- `BaseModel` digunakan untuk mendefinisikan struktur data request/response
- Otomatis memvalidasi tipe data yang diterima

**5. `from typing import List, Optional, Dict`**

- Type hints untuk Python
- `List`: untuk array/list
- `Optional`: untuk field yang boleh kosong (None)
- `Dict`: untuk dictionary/object

**6. Custom Modules**

```python
from document_processor import DocumentProcessor
from text_preprocessor import TextPreprocessor
from tfidf_processor import TFIDFProcessor
from jaccard_similarity import JaccardSimilarity
```

- Import modul-modul yang sudah kita buat sendiri
- Setiap modul punya tanggung jawab spesifik (Single Responsibility Principle)

---

## Pydantic Models (Data Schemas)

```python
class UploadRequest(BaseModel):
    folder_path: str

class SearchRequest(BaseModel):
    query: str
    top_k: Optional[int] = 10

class PreprocessResponse(BaseModel):
    original: str
    case_folding: str
    tokenizing: str
    filtering: str
    stopword_removal: str
    stemming: str
```

### Penjelasan Detail:

**1. `UploadRequest`**

- **Tujuan**: Validasi data yang dikirim frontend saat upload dokumen
- **Field**: `folder_path` (string) - path ke folder dataset
- **Contoh Request Body**:

```json
{
  "folder_path": "C:/Documents/dataset"
}
```

- Jika frontend kirim data dengan format salah, FastAPI otomatis return error 422

**2. `SearchRequest`**

- **Tujuan**: Validasi data untuk endpoint search
- **Field**:
  - `query` (string, required) - kata kunci pencarian
  - `top_k` (int, optional, default=10) - jumlah hasil yang dikembalikan
- **Contoh**:

```json
{
  "query": "pembelajaran mesin",
  "top_k": 5
}
```

**3. `PreprocessResponse`**

- **Tujuan**: Struktur response untuk endpoint `/api/preprocess-demo`
- Menampilkan setiap tahap preprocessing untuk debugging
- Setiap field merepresentasikan 1 tahap preprocessing

---

## Application State Management

```python
class AppState:
    documents = []  # List of raw documents
    processed_docs = []  # List of processed documents
    preprocessor = TextPreprocessor()
    tfidf_processor = TFIDFProcessor()
    ai_formatter = AIFormatter(api_key="...")  # OpenRouter API untuk AI formatting
    is_indexed = False

state = AppState()
```

### Penjelasan Detail:

**Mengapa Perlu State?**

- Backend API bersifat **stateless** (tidak menyimpan data antar request)
- Tapi kita perlu menyimpan:
  1. Dokumen yang sudah di-upload
  2. Hasil preprocessing
  3. TF-IDF matrix
  4. AI Formatter instance
  5. Status apakah sudah di-index atau belum

**Struktur `AppState`:**

**1. `documents = []`**

- Menyimpan dokumen **MENTAH** (belum diproses)
- Struktur setiap dokumen:

```python
{
    'filename': 'dokumen1.txt',
    'file_path': 'C:/dataset/dokumen1.txt',
    'text': 'Isi dokumen mentah...'
}
```

**2. `processed_docs = []`**

- Menyimpan dokumen **SETELAH** preprocessing
- Struktur:

```python
{
    'filename': 'dokumen1.txt',
    'file_path': 'C:/dataset/dokumen1.txt',
    'original_text': 'Isi dokumen mentah...',
    'processed_text': 'isi dokumen mentah',  # Sudah di-stem
    'tokens': ['isi', 'dokumen', 'mentah'],
    'word_count': 3
}
```

**3. `preprocessor = TextPreprocessor()`**

- Instance dari class TextPreprocessor
- Diinisialisasi SEKALI saat aplikasi start
- Berisi Sastrawi stemmer dan stopword remover
- **Efisien**: tidak perlu inisialisasi ulang setiap request

**4. `tfidf_processor = TFIDFProcessor()`**

- Instance dari class TFIDFProcessor
- Menyimpan:
  - Vocabulary (semua kata unik)
  - IDF values
  - TF-IDF matrix untuk semua dokumen

**5. `ai_formatter = AIFormatter(api_key="...")`**

- Instance dari class AIFormatter untuk OpenRouter API
- Model: **Anthropic Claude 3.5 Haiku** (cepat ~4 detik)
- Fungsi: Format text mentah menjadi Markdown terstruktur
- Fitur caching: MD5 hash-based untuk speed up formatting dokumen yang sama
- Base URL: https://openrouter.ai/api/v1

**6. `is_indexed = False`**

- Boolean flag
- `True` = sudah ada dokumen yang di-index, bisa search
- `False` = belum ada dokumen, search akan error

---

## FastAPI Initialization

```python
app = FastAPI(
    title="Sistem Temu Balik Dokumen API",
    description="Backend API untuk search engine lokal dengan TF-IDF dan Generalized Jaccard",
    version="2.0"
)
```

### Penjelasan:

- Membuat instance aplikasi FastAPI
- Metadata ini muncul di **interactive API docs** (`/docs`)
- FastAPI otomatis generate Swagger UI untuk testing API

---

## CORS Configuration

```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Untuk development
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### Penjelasan Detail:

**Apa itu CORS?**

- Browser security feature
- Mencegah website A mengakses API di website B tanpa izin
- Frontend (localhost:3000) dan Backend (localhost:8000) = different origin

**Parameter CORS:**

**1. `allow_origins=["*"]`**

- `["*"]` = terima request dari SEMUA origin
- **Development**: OK untuk kemudahan testing
- **Production**: HARUS spesifik, contoh: `["https://myapp.com"]`
- Jika tidak set, frontend akan dapat error:

```
Access to fetch at 'http://localhost:8000/api/search' from origin 'http://localhost:3000'
has been blocked by CORS policy
```

**2. `allow_credentials=True`**

- Izinkan browser mengirim cookies/credentials
- Diperlukan jika menggunakan authentication

**3. `allow_methods=["*"]`**

- Izinkan semua HTTP methods (GET, POST, PUT, DELETE, dll)
- Bisa dibatasi: `["GET", "POST"]`

**4. `allow_headers=["*"]`**

- Izinkan semua custom headers
- Contoh header: `Authorization`, `Content-Type`

---

## Endpoint 1: Health Check

```python
@app.get("/")
def read_root():
    """Health check endpoint"""
    return {
        "message": "Sistem Temu Balik Dokumen API is running",
        "version": "2.0",
        "status": "active",
        "indexed": state.is_indexed,
        "total_documents": len(state.documents)
    }
```

### Penjelasan Detail:

**1. `@app.get("/")`**

- **Decorator** untuk mendefinisikan endpoint
- `get` = HTTP GET method
- `"/"` = path endpoint (root path)
- URL lengkap: `http://localhost:8000/`

**2. Fungsi `read_root()`**

- Nama fungsi bebas (tidak harus `read_root`)
- Return value otomatis di-convert ke JSON oleh FastAPI

**3. Response Structure**

```json
{
  "message": "Sistem Temu Balik Dokumen API is running",
  "version": "2.0",
  "status": "active",
  "indexed": true, // Apakah sudah ada dokumen
  "total_documents": 100 // Jumlah dokumen yang di-index
}
```

**Kegunaan:**

- Cek apakah backend running
- Cek status indexing
- Debugging

---

## Endpoint 2: Upload dengan Streaming (PALING PENTING)

```python
@app.post("/api/upload-stream")
async def upload_documents_stream(request: UploadRequest):
    """
    Upload dan index dokumen dengan streaming progress
    """
    async def generate_progress():
        try:
            folder_path = request.folder_path

            # Validasi folder exists
            if not os.path.exists(folder_path):
                yield f"data: {json.dumps({'error': 'Folder tidak ditemukan'})}\n\n"
                return
```

### Penjelasan Block Demi Block:

**1. Decorator dan Fungsi**

```python
@app.post("/api/upload-stream")
async def upload_documents_stream(request: UploadRequest):
```

- `@app.post` = endpoint dengan method POST
- `/api/upload-stream` = path endpoint
- `async` = fungsi asynchronous (bisa handle I/O operations dengan efisien)
- `request: UploadRequest` = parameter dengan tipe Pydantic model
  - FastAPI otomatis:
    1. Parse request body JSON
    2. Validasi sesuai schema UploadRequest
    3. Convert ke object Python

**2. Inner Function `generate_progress()`**

```python
async def generate_progress():
```

- **Generator function** yang menghasilkan data secara bertahap
- `async` karena ada `asyncio.sleep()` untuk non-blocking delays
- `yield` = mengirim data satu per satu (seperti return tapi bisa berkali-kali)

**3. Validasi Path**

```python
folder_path = request.folder_path

if not os.path.exists(folder_path):
    yield f"data: {json.dumps({'error': 'Folder tidak ditemukan'})}\n\n"
    return
```

- Ambil path dari request body
- Cek apakah folder exists menggunakan `os.path.exists()`
- Jika tidak ada:
  - `yield` data error dalam format **Server-Sent Events (SSE)**
  - Format SSE: `data: {json}\n\n` (2 newline di akhir)
  - `return` untuk stop execution

---

### Block: Reading Documents

```python
# 1. Baca dokumen
yield f"data: {json.dumps({'status': 'reading', 'message': 'Membaca dokumen dari folder...'})}\n\n"
await asyncio.sleep(0.1)

documents = DocumentProcessor.process_directory(folder_path)

if not documents:
    yield f"data: {json.dumps({'error': 'Tidak ada dokumen valid ditemukan'})}\n\n"
    return

total = len(documents)
yield f"data: {json.dumps({'status': 'found', 'total': total, 'message': f'Menemukan {total} dokumen'})}\n\n"
await asyncio.sleep(0.1)

state.documents = documents
```

**Penjelasan Step-by-Step:**

**Step 1: Kirim Status "Reading"**

```python
yield f"data: {json.dumps({'status': 'reading', 'message': '...'})}\n\n"
```

- Kirim event ke frontend bahwa backend mulai baca dokumen
- Frontend menerima dan bisa tampilkan loading indicator

**Step 2: Delay Non-Blocking**

```python
await asyncio.sleep(0.1)
```

- Delay 0.1 detik (100ms)
- `await` = non-blocking (server bisa handle request lain)
- Berbeda dengan `time.sleep()` yang blocking
- Tujuan: beri waktu frontend untuk render UI

**Step 3: Proses Directory**

```python
documents = DocumentProcessor.process_directory(folder_path)
```

- Panggil static method dari DocumentProcessor
- Method ini akan:
  1. Scan semua file dalam folder
  2. Baca file .txt, .pdf, .docx
  3. Extract teks dari setiap file
  4. Return list of documents

**Step 4: Validasi Hasil**

```python
if not documents:
    yield f"data: {json.dumps({'error': '...'})}\n\n"
    return
```

- Jika tidak ada dokumen valid (list kosong)
- Kirim error ke frontend
- Stop execution

**Step 5: Kirim Jumlah Dokumen**

```python
total = len(documents)
yield f"data: {json.dumps({'status': 'found', 'total': total, ...})}\n\n"
```

- Hitung total dokumen yang ditemukan
- Kirim ke frontend
- Frontend bisa tampilkan: "Ditemukan 100 dokumen"

**Step 6: Simpan ke State**

```python
state.documents = documents
```

- Simpan dokumen mentah ke global state
- Akan digunakan nanti untuk debugging atau re-index

---

### Block: Preprocessing Loop (CORE ALGORITHM)

```python
# 2. Preprocessing dengan progress
processed_docs = []
for idx, doc in enumerate(documents):
    current = idx + 1
    progress = (current / total) * 100

    # Send progress update
    filename = doc['filename']
    message = f'Memproses: {filename} ({current}/{total})'
    progress_data = {
        'status': 'processing',
        'current': current,
        'total': total,
        'progress': progress,
        'filename': filename,
        'message': message
    }
    yield f"data: {json.dumps(progress_data)}\n\n"

    processed_text, tokens = state.preprocessor.preprocess(doc['text'])
    processed_docs.append({
        'filename': doc['filename'],
        'file_path': doc['file_path'],
        'original_text': doc['text'],
        'processed_text': processed_text,
        'tokens': tokens,
        'word_count': len(tokens)
    })

    await asyncio.sleep(0.05)  # Small delay untuk smooth progress

state.processed_docs = processed_docs
```

**Penjelasan Detail:**

**1. Inisialisasi List Hasil**

```python
processed_docs = []
```

- List kosong untuk menampung dokumen yang sudah diproses
- Akan di-append setiap kali selesai proses 1 dokumen

**2. Loop Through Documents**

```python
for idx, doc in enumerate(documents):
```

- `enumerate()` = dapat index DAN value
- `idx` = index (0, 1, 2, ...)
- `doc` = object dokumen {'filename': ..., 'text': ...}

**3. Hitung Progress**

```python
current = idx + 1  # Karena idx mulai dari 0
progress = (current / total) * 100
```

- `current` = nomor dokumen yang sedang diproses (1-100)
- `progress` = persentase (0-100)
- Contoh: dokumen ke-25 dari 100 = 25%

**4. Buat Progress Data**

```python
filename = doc['filename']
message = f'Memproses: {filename} ({current}/{total})'
progress_data = {
    'status': 'processing',
    'current': current,
    'total': total,
    'progress': progress,
    'filename': filename,
    'message': message
}
```

- Object berisi informasi progress
- Frontend akan parse ini dan tampilkan:
  - Progress bar: 25%
  - Text: "Memproses: dokumen25.txt (25/100)"

**5. Kirim Progress ke Frontend**

```python
yield f"data: {json.dumps(progress_data)}\n\n"
```

- Kirim via SSE
- Frontend menerima REAL-TIME tanpa perlu polling
- Frontend bisa update UI setiap kali dapat event

**6. PREPROCESSING (CORE)**

```python
processed_text, tokens = state.preprocessor.preprocess(doc['text'])
```

- Ini yang paling penting!
- `state.preprocessor` = instance TextPreprocessor
- `.preprocess()` melakukan 6 tahap:
  1. Case Folding
  2. Tokenizing
  3. Filtering
  4. Stopword Removal (Sastrawi)
  5. Stemming (Sastrawi)
  6. Final Tokenization
- Return:
  - `processed_text`: string text yang sudah di-stem
  - `tokens`: list of words (array)

**Contoh Transformasi:**

```
Input: "Pembelajaran Mesin adalah Cabang dari Artificial Intelligence"

Setelah preprocessing:
processed_text: "ajar mesin cabang artificial intelligence"
tokens: ['ajar', 'mesin', 'cabang', 'artificial', 'intelligence']
```

**7. Simpan Hasil**

```python
processed_docs.append({
    'filename': doc['filename'],
    'file_path': doc['file_path'],
    'original_text': doc['text'],
    'processed_text': processed_text,
    'tokens': tokens,
    'word_count': len(tokens)
})
```

- Buat object baru dengan data lengkap
- Simpan:
  - Metadata (filename, path)
  - Text original (untuk preview)
  - Text processed (untuk TF-IDF)
  - Tokens (untuk analisis)
  - Word count (untuk statistik)

**8. Delay untuk Smooth UI**

```python
await asyncio.sleep(0.05)  # 50ms
```

- Delay kecil agar frontend bisa render progress
- Tanpa ini, progress terlalu cepat dan UI jadi lag
- 50ms = sweet spot antara kecepatan dan smooth UI

**9. Simpan ke State**

```python
state.processed_docs = processed_docs
```

- Simpan SEMUA dokumen yang sudah diproses
- State ini akan digunakan untuk TF-IDF

---

### Block: TF-IDF Matrix Building

```python
# 3. Build TF-IDF matrix
yield f"data: {json.dumps({'status': 'tfidf', 'message': 'Membuat TF-IDF matrix...'})}\n\n"
await asyncio.sleep(0.1)

texts = [doc['processed_text'] for doc in processed_docs]
state.tfidf_processor.fit_transform(texts)
state.is_indexed = True
```

**Penjelasan:**

**1. Kirim Status**

```python
yield f"data: {json.dumps({'status': 'tfidf', 'message': '...'})}\n\n"
```

- Informasikan frontend bahwa mulai build TF-IDF

**2. Extract Processed Texts**

```python
texts = [doc['processed_text'] for doc in processed_docs]
```

- **List comprehension**
- Ambil hanya field `processed_text` dari setiap dokumen
- Hasil: list of strings

```python
texts = [
    "ajar mesin cabang artificial intelligence",
    "deep learn neural network",
    ...
]
```

**3. Build TF-IDF Matrix**

```python
state.tfidf_processor.fit_transform(texts)
```

- Method dari TFIDFProcessor
- **fit**: Buat vocabulary dan hitung IDF
- **transform**: Hitung TF-IDF untuk setiap dokumen
- Hasil disimpan di dalam `tfidf_processor` object:
  - `vocabulary`: semua kata unik
  - `idf_values`: IDF setiap kata
  - `tfidf_matrix`: matrix TF-IDF semua dokumen

**4. Set Flag Indexed**

```python
state.is_indexed = True
```

- Mark bahwa sistem sudah siap untuk search
- Jika belum `True`, endpoint search akan return error

---

### Block: Send Completion Response

```python
# 4. Complete
result = {
    "status": "complete",
    "message": f"Berhasil mengindex {len(documents)} dokumen",
    "total_documents": len(documents),
    "documents": [
        {
            "id": idx,
            "filename": doc['filename'],
            "original_text_preview": doc['original_text'][:200] + "...",
            "processed_text_preview": doc['processed_text'][:200] + "...",
            "word_count": doc['word_count']
        }
        for idx, doc in enumerate(processed_docs)
    ]
}

yield f"data: {json.dumps(result)}\n\n"
```

**Penjelasan:**

**1. Buat Response Object**

- `status: "complete"` = signal bahwa proses selesai
- `message` = pesan sukses
- `total_documents` = jumlah dokumen
- `documents` = list dokumen dengan preview

**2. List Comprehension untuk Documents**

```python
"documents": [
    {...}
    for idx, doc in enumerate(processed_docs)
]
```

- Buat list of objects
- Setiap object = 1 dokumen dengan preview
- `[:200] + "..."` = ambil 200 karakter pertama saja (preview)
  - Menghemat bandwidth
  - Tidak perlu kirim full text ke frontend

**3. Yield Final Result**

```python
yield f"data: {json.dumps(result)}\n\n"
```

- Kirim response terakhir
- Frontend tahu proses selesai karena `status: "complete"`
- Frontend bisa tampilkan list dokumen di UI

---

### Block: Error Handling

```python
except Exception as e:
    yield f"data: {json.dumps({'error': str(e)})}\n\n"
```

**Penjelasan:**

- `try-except` untuk catch semua error
- Jika ada error di mana saja:
  1. Convert error ke string: `str(e)`
  2. Kirim ke frontend dalam format SSE
  3. Frontend bisa tampilkan error message

**Contoh Error yang Mungkin:**

- File corrupt (tidak bisa dibaca)
- Out of memory (terlalu banyak dokumen)
- Permission denied (tidak bisa akses folder)

---

### Block: Return StreamingResponse

```python
return StreamingResponse(
    generate_progress(),
    media_type="text/event-stream",
    headers={
        "Cache-Control": "no-cache",
        "Connection": "keep-alive",
    }
)
```

**Penjelasan Detail:**

**1. `StreamingResponse`**

- FastAPI class untuk streaming response
- Berbeda dengan normal response yang kirim semua data sekaligus
- Streaming = kirim data bertahap

**2. `generate_progress()`**

- Generator function yang kita buat
- FastAPI otomatis call function ini
- Setiap `yield` akan dikirim ke client

**3. `media_type="text/event-stream"`**

- MIME type untuk **Server-Sent Events (SSE)**
- Browser akan otomatis handle sebagai event stream
- Frontend bisa listen dengan `EventSource` API

**4. Headers**

```python
"Cache-Control": "no-cache"  # Jangan cache response
"Connection": "keep-alive"   # Keep connection open
```

- `no-cache`: Browser tidak boleh cache streaming data
- `keep-alive`: HTTP connection tetap terbuka untuk menerima events

---

## Endpoint 3: Search Documents

```python
@app.post("/api/search")
def search_documents(request: SearchRequest):
    """
    Cari dokumen berdasarkan query
    """
    if not state.is_indexed:
        raise HTTPException(status_code=400, detail="Belum ada dokumen yang di-index")

    try:
        # Preprocess query
        query_processed, query_tokens = state.preprocessor.preprocess(request.query)

        # Transform query ke TF-IDF vector
        query_vector = state.tfidf_processor.transform_query(query_processed)

        # Hitung similarity dengan semua dokumen
        doc_vectors = state.tfidf_processor.tfidf_matrix
        doc_info = [
            {
                'filename': doc['filename'],
                'original_text': doc['original_text'],
                'word_count': doc['word_count']
            }
            for doc in state.processed_docs
        ]

        results = JaccardSimilarity.rank_documents(
            query_vector, doc_vectors, doc_info
        )

        # Ambil top-k
        top_results = results[:request.top_k]

        return {
            "query_original": request.query,
            "query_processed": query_processed,
            "query_tokens": query_tokens,
            "total_results": len(results),
            "results": top_results
        }

    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Penjelasan Block Demi Block:

**1. Decorator**

```python
@app.post("/api/search")
def search_documents(request: SearchRequest):
```

- POST endpoint di path `/api/search`
- Parameter: `SearchRequest` dengan field `query` dan `top_k`

**2. Validasi Index**

```python
if not state.is_indexed:
    raise HTTPException(status_code=400, detail="Belum ada dokumen yang di-index")
```

- Cek apakah sudah ada dokumen di-index
- Jika belum, return HTTP 400 (Bad Request)
- Frontend akan dapat error message

**3. Preprocess Query**

```python
query_processed, query_tokens = state.preprocessor.preprocess(request.query)
```

- **PENTING**: Query harus melalui preprocessing yang SAMA dengan dokumen
- Contoh:

```
Input: "Apa itu Pembelajaran Mesin?"
Output:
- query_processed: "ajar mesin"
- query_tokens: ['ajar', 'mesin']
```

**4. Transform Query ke TF-IDF Vector**

```python
query_vector = state.tfidf_processor.transform_query(query_processed)
```

- Convert query text menjadi TF-IDF vector
- Vector ini akan dibandingkan dengan vector dokumen
- Ukuran vector = jumlah kata dalam vocabulary
- Contoh: jika vocabulary ada 1000 kata, vector berukuran 1000

**5. Prepare Document Data**

```python
doc_vectors = state.tfidf_processor.tfidf_matrix
doc_info = [
    {
        'filename': doc['filename'],
        'original_text': doc['original_text'],
        'word_count': doc['word_count']
    }
    for doc in state.processed_docs
]
```

- `doc_vectors`: TF-IDF matrix (numpy array)
- `doc_info`: metadata dokumen untuk response

**6. Hitung Similarity & Ranking**

```python
results = JaccardSimilarity.rank_documents(
    query_vector, doc_vectors, doc_info
)
```

- Method dari JaccardSimilarity class
- Hitung similarity antara query dan SETIAP dokumen
- Sort dokumen berdasarkan similarity (tertinggi dulu)
- Return list dokumen dengan rank

**7. Ambil Top-K**

```python
top_results = results[:request.top_k]
```

- Slicing list untuk ambil top-k results
- Default: top 10
- Jika user request `top_k=5`, ambil 5 teratas

**8. Return Response**

```python
return {
    "query_original": request.query,
    "query_processed": query_processed,
    "query_tokens": query_tokens,
    "total_results": len(results),
    "results": top_results
}
```

- Frontend dapat semua informasi:
  - Query asli dan yang sudah diproses
  - Tokens untuk debugging
  - Total hasil (bukan cuma top-k)
  - Top-k results dengan ranking

---

## Endpoint 4: Get All Documents

```python
@app.get("/api/documents")
def get_all_documents():
    """Get list semua dokumen yang sudah diindex"""
    try:
        if not state.is_indexed:
            return {
                "status": "success",
                "total": 0,
                "documents": []
            }

        return {
            "status": "success",
            "total": len(state.processed_docs),
            "documents": [
                {
                    "id": idx,
                    "filename": doc['filename'],
                    "original_text_preview": doc['original_text'][:200] + "...",
                    "processed_text_preview": doc['processed_text'][:200] + "...",
                    "word_count": doc['word_count']
                }
                for idx, doc in enumerate(state.processed_docs)
            ]
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")
```

### Penjelasan:

**Tujuan**: Endpoint untuk mendapatkan list semua dokumen yang sudah di-index (untuk ditampilkan di frontend)

**Return Value:**

- `total`: Jumlah total dokumen
- `documents`: List dokumen dengan preview (200 karakter pertama)
- Setiap dokumen punya `id` yang merupakan index dalam array (digunakan untuk fetch detail)

**Use Case:**

- Tampilkan card dokumen di tab "Dataset & Indexing"
- User bisa klik card untuk lihat detail lengkap

---

## Endpoint 5: Get Document Detail

```python
@app.get("/api/document/{doc_id}")
def get_document_detail(doc_id: int):
    """Get detail lengkap dokumen tertentu untuk visualisasi di modal"""
    try:
        if not state.is_indexed or doc_id >= len(state.processed_docs):
            raise HTTPException(status_code=404, detail="Dokumen tidak ditemukan")

        doc = state.processed_docs[doc_id]

        return {
            "status": "success",
            "document": {
                "id": doc_id,
                "filename": doc['filename'],
                "file_path": doc['file_path'],
                "original_text": doc['original_text'],
                "processed_text": doc['processed_text'],
                "tokens": doc['tokens'],
                "word_count": doc['word_count']
            }
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")
```

### Penjelasan Detail:

**1. Path Parameter**

```python
@app.get("/api/document/{doc_id}")
def get_document_detail(doc_id: int):
```

- `{doc_id}` = path parameter dinamis
- FastAPI otomatis convert string ke int
- Contoh URL: `/api/document/0`, `/api/document/25`

**2. Validasi**

```python
if not state.is_indexed or doc_id >= len(state.processed_docs):
    raise HTTPException(status_code=404, detail="Dokumen tidak ditemukan")
```

- Cek apakah sudah indexed
- Cek apakah doc_id valid (tidak out of range)
- Return 404 jika tidak ditemukan

**3. Response Structure**

- Return **FULL TEXT** (bukan preview)
- Include:
  - Original text (mentah)
  - Processed text (sudah di-stem)
  - Tokens array
  - Metadata (filename, path, word count)

**Use Case:**

- User klik card dokumen ‚Üí Modal terbuka
- Modal tampilkan 3 tabs:
  - **Original**: Text mentah
  - **Processed**: Text setelah preprocessing dengan tokens
  - **Statistics**: Statistik dokumen (word count, unique tokens, dll)

---

## Endpoint 6: AI Text Formatting (‚≠ê FITUR BARU)

```python
@app.get("/api/document/{doc_id}/format-ai")
def format_document_with_ai(doc_id: int):
    """
    Format dokumen dengan AI (Anthropic Claude 3.5 Haiku via OpenRouter)
    Mengubah text mentah menjadi Markdown yang terstruktur
    """
    try:
        if not state.is_indexed or doc_id >= len(state.processed_docs):
            raise HTTPException(status_code=404, detail="Dokumen tidak ditemukan")

        doc = state.processed_docs[doc_id]
        original_text = doc['original_text']

        # Format menggunakan AI (dengan cache)
        result = state.ai_formatter.format_text(original_text, use_cache=True)

        return {
            "status": "success",
            "document_id": doc_id,
            "filename": doc['filename'],
            "formatted_text": result['formatted_text'],
            "from_cache": result.get('from_cache', False),
            "original_length": result['original_length'],
            "formatted_length": result['formatted_length'],
            "cache_key": result.get('cache_key'),
            "error": result.get('error')
        }
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error: {str(e)}")
```

### Penjelasan Detail:

**üéØ Tujuan:**

- Mengubah dokumen text mentah (wall of text) menjadi Markdown terstruktur
- AI mendeteksi struktur dokumen dan menambahkan formatting:
  - Headings (#, ##, ###)
  - Bold (**text**) untuk istilah penting
  - Bullet points (-) untuk list
  - Numbered lists (1., 2., 3.)
  - Horizontal rules (---) untuk pemisah section

**ü§ñ AI Provider & Model:**

- **Provider**: OpenRouter API (https://openrouter.ai)
- **Model**: `anthropic/claude-3.5-haiku`
- **Kecepatan**: ~3-4 detik per dokumen
- **Mengapa Claude Haiku?**
  - Sangat cepat (lebih cepat dari GPT-4)
  - Reliable dalam mengikuti instruksi formatting
  - Cost-effective
  - Alternatif yang dicoba:
    - ‚ùå Google Gemini Flash 1.5: endpoint not found
    - ‚ùå GLM 4.7: terlalu lambat (3-5 menit)
    - ‚úÖ Claude 3.5 Haiku: perfect balance

**üíæ Caching System:**

```python
result = state.ai_formatter.format_text(original_text, use_cache=True)
```

- Menggunakan **MD5 hash** dari original text sebagai cache key
- Cache disimpan di: `backend/cache/formatted_docs/{md5_hash}.json`
- **First request**: Hit API (~4 detik)
- **Subsequent requests**: Load dari cache (<100ms)
- Cache persistent (tetap ada setelah restart server)

**üìä Response Fields:**

**1. `formatted_text`**

- Text hasil formatting dalam format Markdown
- Contoh:

```markdown
# Machine Learning

**Machine Learning** adalah cabang dari **Artificial Intelligence** yang fokus pada pengembangan algoritma yang dapat belajar dari data.

## Jenis-Jenis Machine Learning

Ada beberapa jenis:

- **Supervised Learning**
- **Unsupervised Learning**
- **Reinforcement Learning**

---
```

**2. `from_cache`**

- Boolean: `true` jika dari cache, `false` jika dari API
- Frontend bisa tampilkan indicator

**3. `original_length` & `formatted_length`**

- Jumlah karakter sebelum dan setelah formatting
- Biasanya formatted lebih panjang karena ada markup Markdown

**4. `cache_key`**

- MD5 hash untuk debugging
- Bisa digunakan untuk manually clear cache

**üé® Frontend Integration:**

- Button "Format dengan AI ‚ú®" di modal
- Loading state saat formatting
- ReactMarkdown render hasil dengan custom styling:
  - Headings dengan font size berbeda
  - Lists dengan proper bullets
  - Bold text dengan color primary
  - Syntax highlighting untuk code blocks

**‚ö° Performance:**

- **Without Cache**: ~3.7 seconds
- **With Cache**: ~0.05 seconds (75x faster!)
- Cache hit rate setelah beberapa usage: >90%

**üîê Security:**

- API key disimpan di backend (tidak exposed ke frontend)
- Rate limiting bisa ditambahkan di production
- Error handling: jika AI gagal, return original text

---

## Ringkasan Alur Kerja

### Upload Flow:

```
1. Frontend kirim folder path
2. Backend validasi path
3. Backend baca semua file dalam folder (DocumentProcessor)
4. Loop setiap file:
   a. Kirim progress ke frontend (SSE)
   b. Preprocessing dengan Sastrawi (6 tahap)
   c. Simpan hasil
5. Build TF-IDF matrix untuk SEMUA dokumen
6. Set flag is_indexed = True
7. Kirim completion response dengan preview dokumen
```

### Search Flow:

```
1. Frontend kirim query
2. Backend validasi sudah indexed atau belum
3. Preprocess query (sama seperti dokumen)
4. Transform query ke TF-IDF vector
5. Hitung Generalized Jaccard Similarity dengan semua dokumen
6. Rank dokumen (tertinggi dulu)
7. Ambil top-k hasil
8. Return ke frontend
```

### Document Detail Flow:

```
1. User klik card dokumen (dari hasil upload atau search)
2. Frontend request detail dengan doc_id
3. Backend return full text + metadata
4. Frontend tampilkan modal dengan 3 tabs
5. (Optional) User klik "Format dengan AI"
   a. Frontend request AI formatting
   b. Backend cek cache (MD5 hash)
   c. Jika ada di cache: return langsung
   d. Jika tidak: call OpenRouter API
   e. Simpan hasil ke cache
   f. Return formatted Markdown
6. Frontend render Markdown dengan ReactMarkdown
7. Search query di-highlight di konten (kuning)
```

---

## API Endpoints Summary

| Method | Endpoint                       | Deskripsi                    | Request          | Response                  |
| ------ | ------------------------------ | ---------------------------- | ---------------- | ------------------------- |
| GET    | `/`                            | Health check                 | -                | Status API + indexed info |
| POST   | `/api/upload-stream`           | Upload & index dokumen (SSE) | `{folder_path}`  | Streaming progress        |
| POST   | `/api/search`                  | Cari dokumen                 | `{query, top_k}` | Ranked documents          |
| GET    | `/api/documents`               | List semua dokumen           | -                | Array dokumen + preview   |
| GET    | `/api/document/{id}`           | Detail dokumen               | doc_id           | Full text + tokens        |
| GET    | `/api/document/{id}/format-ai` | Format dengan AI             | doc_id           | Markdown formatted text   |

---

## Tips Optimasi

**1. Memory Management**

- Untuk dataset besar (>1000 dokumen), pertimbangkan:
  - Batch processing
  - Database untuk storage
  - Caching results

**2. Performance**

- TF-IDF matrix bisa sangat besar (sparse matrix)
- Gunakan scipy sparse matrix untuk efisiensi
- Preprocessing bisa di-parallelize dengan multiprocessing

**3. Error Handling**

- Tambah try-catch per file saat baca dokumen
- Jangan biarkan 1 file corrupt menghentikan seluruh proses

**4. Production Considerations**

- Ganti in-memory state dengan database (Redis/PostgreSQL)
- Add authentication
- Rate limiting untuk prevent abuse
- Logging untuk debugging

---

Ini adalah penjelasan detail untuk `main.py`. Apakah Anda ingin saya lanjutkan dengan file-file lainnya?
